{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0fc180b7-2770-4176-a658-0e8f3b53b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import coint\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import time\n",
    "import requests\n",
    "import itertools\n",
    "from pykalman import KalmanFilter\n",
    "import pyarrow.parquet as pq\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046b2687-1475-43b4-8b49-3aebeeedf49f",
   "metadata": {},
   "source": [
    "Reading the Candlestick parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b3fd077-358c-453f-89ed-5a38e58a4f8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"Stocks_candle_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f579d733-55b8-4c8b-888b-82c9e366d434",
   "metadata": {},
   "source": [
    "Converting the index into datetime instead of string, making the index timezone naive to remove inconsistency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b5c3d4d-2fdc-4da6-a738-14c66352279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = pd.to_datetime(df.index)        # convert to datetime\n",
    "df.index = df.index.tz_localize(None)      # remove timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff185a-de5e-46cc-8fcb-cfa832923490",
   "metadata": {},
   "source": [
    "Using yfinance to categorize all stocks into respective industries, since there are ~468 stocks -> this leads to 468C2 pairs ~ 110k pairs which are computationally very expensive.\n",
    "Cointegration is more likely within the same industries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c4f661d3-638b-48e2-8615-c7da691fbb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sector_info(ticker: str, max_retries: int = 5, base_delay: float = 2.0):\n",
    "    \"\"\"\n",
    "    Fetch sector and industry for a given NSE ticker using yfinance,\n",
    "    with retry and rate-limiting handling.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            \n",
    "            info = data.get_info()\n",
    "            \n",
    "            sector = info.get(\"sector\", \"Unknown\")\n",
    "            industry = info.get(\"industry\", \"Unknown\")\n",
    "            \n",
    "            # if both are unknown, might be a bad response â†’ retry\n",
    "            if sector == \"Unknown\" and industry == \"Unknown\" and attempt < max_retries - 1:\n",
    "                raise ValueError(\"Empty response, retrying...\")\n",
    "            \n",
    "            return sector, industry\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if \"429\" in str(e):  # Too Many Requests\n",
    "                sleep_time = base_delay * (2 ** attempt)  # exponential backoff\n",
    "                print(f\"Rate limit hit for {ticker}, sleeping {sleep_time:.1f}s...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                print(f\"HTTP error for {ticker}: {e}\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            # Generic error, retry if attempts remain\n",
    "            print(f\"Error fetching {ticker} (attempt {attempt+1}/{max_retries}): {e}\")\n",
    "            time.sleep(base_delay * (attempt + 1))\n",
    "\n",
    "    # If all retries fail\n",
    "    return \"Unknown\", \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cc6fce3b-b684-49e3-9f8e-214386a89ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = df['Stock'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e942b35-e649-4eb4-b56d-29d89d4df1e5",
   "metadata": {},
   "source": [
    "yfinance requires \".NS\" postscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec0236e-c20c-4d93-af58-4ceff6c28f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    ticker=ticker+\".NS\"\n",
    "    data = yf.Ticker(ticker)\n",
    "    industry = data.info.get(\"industry\", \"Unknown\")\n",
    "    records.append({\n",
    "        \"Stock\": ticker,\n",
    "        \"Industry\": industry\n",
    "    })\n",
    "\n",
    "industry_df = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda62104-9d65-44e4-85d6-c5f122884d97",
   "metadata": {},
   "source": [
    "I am initially testing cointegration on January 2022 data only -> This is done to avoid look-ahead bias. I have made no trades during this period.\n",
    "If I test cointegration on the entire data, I am essentially using the entire data (which I won't have access to in actual situations).\n",
    "I have added the part to periodically retest cointegration to ensure that all the selected pairs are still have that relationship, or to add new pairs that might later become cointegrated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0989afb-b990-4a12-aac6-e7085577679a",
   "metadata": {},
   "source": [
    "Since it is quite difficult to work with the original data format (date x stock gives a unique row) -> I have created a function to pivot the table based on price_col (close/open), giving all the stocks as separate columns. It makes it easier to directly compare corrosponding relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7270d1f-e3c6-41a8-b144-5b652f1d3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_wide_df(df, start_date, end_date, price_col=\"close\"):\n",
    "    # Ensure datetime is a column\n",
    "    df = df.reset_index().rename(columns={\"date\": \"datetime\"})\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "    \n",
    "    # Filter by date range\n",
    "    mask = (df[\"datetime\"] >= start_date) & (df[\"datetime\"] <= end_date)\n",
    "    df = df.loc[mask]\n",
    "\n",
    "    # Pivot: rows = datetime, columns = stocks, values = close\n",
    "    wide_df = df.pivot_table(index=\"datetime\", columns=\"Stock\", values=price_col)\n",
    "\n",
    "    # Sort index and forward-fill missing values\n",
    "    wide_df = wide_df.sort_index().ffill()\n",
    "\n",
    "    return wide_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "87d4beaf-626f-4837-96d9-cb4e24a52b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close = prepare_wide_df(df, \"2022-01-01\", \"2022-01-31\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "74237a75-ed48-4422-98d1-d71ce65637eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_open = prepare_wide_df(df, \"2022-01-01\", \"2022-01-31\", \"open\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73e1d563-37b4-438f-8f4d-d88158f5d826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 468)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_df_close.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d294a248-4e16-41d7-9222-90d4c752f813",
   "metadata": {},
   "source": [
    "To further reduce computation, I am first filtering the stocks based on correlation, within industries. Calculating correlation is significantly faster than calculating cointegration, and as a rule of thumb, cointegration is more likely in highly correlated pairs rather than uncorrelated pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b12996fd-429a-4b80-afa6-f7a41be59183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_correlation(wide_df, threshold=0.7, method=\"pearson\"):\n",
    "    corr_matrix = wide_df.corr(method=method)\n",
    "    correlated_pairs = []\n",
    "    \n",
    "    for i, j in itertools.combinations(corr_matrix.columns, 2):\n",
    "        corr_val = corr_matrix.loc[i, j]\n",
    "        if abs(corr_val) >= threshold:\n",
    "            correlated_pairs.append((i, j, corr_val))\n",
    "    \n",
    "    return correlated_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1001b5-4362-463b-b4e0-8f1034a2d628",
   "metadata": {},
   "source": [
    "Engle-Granger test for cointegration with signficance level of 5% as default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "de3e81f8-df47-4e5b-9562-997abe154dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cointegration(wide_df, stock_pairs=None, significance=0.05):\n",
    "    \"\"\"\n",
    "    if stock_pairs is None:\n",
    "        stock_pairs = list(itertools.combinations(wide_df.columns, 2))\n",
    "    \n",
    "    coint_pairs = []\n",
    "    \n",
    "    for stock1, stock2 in stock_pairs:\n",
    "        series1 = wide_df[stock1].dropna()\n",
    "        series2 = wide_df[stock2].dropna()\n",
    "        \n",
    "        if len(series1) == 0 or len(series2) == 0:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            score, pvalue, _ = coint(series1, series2)\n",
    "            if pvalue < significance:\n",
    "                coint_pairs.append((stock1, stock2, pvalue))\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    return coint_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "44c57961-875c-44bd-a8a4-9b492b314021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter by correlation\n",
    "corr_pairs = filter_correlation(wide_df_close, threshold=0.7)\n",
    "#print(\"Highly correlated pairs:\", corr_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a44afc8a-4534-481f-85ae-045c817d5a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Filter by cointegration (using correlated pairs only)\n",
    "pairs_for_coint = [(s1, s2) for s1, s2, _ in corr_pairs]\n",
    "coint_pairs = filter_cointegration(wide_df_close, stock_pairs=pairs_for_coint, significance=0.05)\n",
    "#print(\"Cointegrated pairs:\", coint_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48cab17-80a4-4eea-9e1f-6610c7644449",
   "metadata": {},
   "source": [
    "Because of the insanely large size of the dataset, I faced memory overload issues on Jupyter as well as Colab, since all computation was being done locally with no support for parallel processing (Pyspark could have solved the issue), I had to get a workaround which was to divide the entire dataframe month-wise, now the size of any particular dataframe is workable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1b1b126e-3085-4b0e-9c9d-76c7ff9ec133",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_jan22 = prepare_wide_df(df, \"2022-01-01\", \"2022-01-31\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e89d277c-bcf5-4966-a12e-a36117e50774",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_feb22 = prepare_wide_df(df, \"2022-02-01\", \"2022-02-28\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bef1740f-81aa-437e-a061-0b0d363cfa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_mar22 = prepare_wide_df(df, \"2022-03-01\", \"2022-03-31\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b6090cbd-5e98-49f6-9472-c89b1f1b5528",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_apr22 = prepare_wide_df(df, \"2022-04-01\", \"2022-04-30\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d658375e-e7fc-4b44-8280-58e655de43a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_may22 = prepare_wide_df(df, \"2022-05-01\", \"2022-05-31\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1bc22e07-035d-40af-8269-d51fc519e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_jun22 = prepare_wide_df(df, \"2022-06-01\", \"2022-06-30\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "475c2f3f-dc2c-4d7d-ad5e-57303ef2406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_jul22 = prepare_wide_df(df, \"2022-07-01\", \"2022-07-31\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "77f834b0-e334-4695-ac26-4916fb706987",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_aug22 = prepare_wide_df(df, \"2022-08-01\", \"2022-08-31\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e41ac49e-db68-4e3d-aaf1-515abe2e6a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_sep22 = prepare_wide_df(df, \"2022-09-01\", \"2022-09-30\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2c7923dc-aee2-4ec5-a73b-19257b9ffb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_oct22 = prepare_wide_df(df, \"2022-10-01\", \"2022-10-31\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "31e50965-e6db-40e4-94ad-010dda882086",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_nov22 = prepare_wide_df(df, \"2022-11-01\", \"2022-11-30\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "57df2c2d-1ab6-4ec3-ad49-4f227822777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_dec22 = prepare_wide_df(df, \"2022-12-01\", \"2022-12-31\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a2253b3-8499-49aa-acb9-bfdf5bba8182",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_jan23 = prepare_wide_df(df, \"2023-01-01\", \"2023-01-31\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b89df49d-1b08-47ee-88da-623bde6d8870",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_feb23 = prepare_wide_df(df, \"2023-02-01\", \"2023-02-28\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3943ef6-17db-46f1-923d-6d1c817e62fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_mar23 = prepare_wide_df(df, \"2023-03-01\", \"2023-03-31\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe7ee5cb-d00d-47bc-985f-0587d34e0f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_apr23 = prepare_wide_df(df, \"2023-04-01\", \"2023-04-30\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6602382a-c650-46cb-81db-7aab02696f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_may23 = prepare_wide_df(df, \"2023-05-01\", \"2023-05-31\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46d95f47-9580-4313-afa4-4c3f1967407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_jun23 = prepare_wide_df(df, \"2023-06-01\", \"2023-06-30\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba8adb33-aa92-4249-83c5-752bc06ba187",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_jul23 = prepare_wide_df(df, \"2023-07-01\", \"2023-07-31\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bf11f60-56b0-498c-8c7e-43eadb7279c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_aug23 = prepare_wide_df(df, \"2023-08-01\", \"2023-08-31\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64c3da67-065e-4262-806e-0fde75fefe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_sep23 = prepare_wide_df(df, \"2023-09-01\", \"2023-09-30\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "644de510-1cf6-4588-aed7-00cbb65cdabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_oct23 = prepare_wide_df(df, \"2023-10-01\", \"2023-10-31\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a92dbef3-4c31-44bf-864e-4c2eb7d7a288",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_nov23 = prepare_wide_df(df, \"2023-11-01\", \"2023-11-30\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93b7b907-4ed3-40e7-a3a0-71bbf30844ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df_close_dec23 = prepare_wide_df(df, \"2023-12-01\", \"2023-12-31\", \"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9c94b0e2-e95d-48ad-83d6-4642da5f820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_dfs = [wide_df_close_jan22, wide_df_close_feb22, wide_df_close_mar22, wide_df_close_apr22, wide_df_close_may22, wide_df_close_jun22, wide_df_close_jul22, wide_df_close_aug22, wide_df_close_sep22, wide_df_close_oct22, wide_df_close_nov22, wide_df_close_dec22, wide_df_close_jan23, wide_df_close_feb23, wide_df_close_mar23, wide_df_close_apr23, wide_df_close_may23, wide_df_close_jun23, wide_df_close_jul23, wide_df_close_aug23, wide_df_close_sep23, wide_df_close_oct23, wide_df_close_nov23, wide_df_close_dec23]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6df07a-0e2d-49f4-a9b7-749b4e0c3231",
   "metadata": {},
   "source": [
    "# Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc21bc2-242c-41ec-b93c-5a338bb98492",
   "metadata": {},
   "source": [
    "Below is the implementation of the strategy as a single class. The strategy uses Kalman Filter to calculate dynamic hedge ratio, using the data of the last 3 days, which would usually be sufficient and relevant.\n",
    "\n",
    "The signals are generated through Z-Score. \n",
    "Close prices are used for all trades since they are more stable.\n",
    "\n",
    "To avoid look-ahead bias, at time 't', the close price of 't-1' is used to make a trade, since the close of 't' will not be available.\n",
    "Since NSE does not allow interday short-selling, all open positions are closed at EOD.\n",
    "\n",
    "Since the hedge ratio gives the relationship in the spread, and not the actual volume, the volume for each pair is determined to maintain dollar-neutral exposure within a fixed capital allocation per pair. Stock 1â€™s size is set so the total notional stays within the allocated capital, and stock 2â€™s size is scaled by the hedge ratio to hedge the position. This ensures the total investment per pair respects the portfolio allocation while reflecting the spread relationship, with transaction costs applied for realistic P&L.\n",
    "\n",
    "A transaction fee of 0.005% (parameter) has been added to make simulation more realistic.\n",
    "\n",
    "For risk management, stop loss has been added (0.05 Z-Score), to handle large market down conditions.\n",
    "\n",
    "Earlier I had planned to use a copula-based strategy, but I was limited due Python not having any extensive copula library, and Colab not being able to install the copula library in R. \n",
    "The R code for the main implementation has been provided at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d7f643fc-27b3-4dc8-b3ca-123a49f3096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "class PairsTradingBacktest:\n",
    "    def __init__(self, monthly_dfs_dict, pairs, initial_capital=1_000_000, window=60, stop_loss=0.02, fee_rate=0.0005):\n",
    "        self.monthly_dfs = monthly_dfs_dict\n",
    "        self.pairs = pairs\n",
    "        self.initial_capital = initial_capital\n",
    "        self.window = window\n",
    "        self.stop_loss = stop_loss\n",
    "        self.fee_rate = fee_rate\n",
    "\n",
    "    def _kalman_hedge_ratio(self, y, x):\n",
    "        # Reshaping x to (n_timesteps, 1, 1) to match pykalman's expectation\n",
    "        x = x[:, np.newaxis, np.newaxis]\n",
    "        \n",
    "        kf = KalmanFilter(\n",
    "            transition_matrices=[[1]],\n",
    "            observation_matrices=x,  # Now correctly shaped as (200, 1, 1)\n",
    "            initial_state_mean=0,\n",
    "            initial_state_covariance=1,\n",
    "            observation_covariance=1,\n",
    "            transition_covariance=1e-5\n",
    "        )\n",
    "        state_means, _ = kf.filter(y)\n",
    "        return state_means.flatten()[-1]\n",
    "\n",
    "    def _trade_pair(self, y, x, dates):\n",
    "        pnl = np.zeros(len(y))\n",
    "        position = 0  \n",
    "        entry_price = 0\n",
    "        \n",
    "        # Extract dates for easy comparison\n",
    "        day_change_indices = dates.to_series().diff().dt.days.ne(0)\n",
    "        \n",
    "        for t in range(self.window, len(y)):\n",
    "            hedge_ratio = self._kalman_hedge_ratio(y[t-self.window:t], x[t-self.window:t])\n",
    "            spread = y[t] - hedge_ratio * x[t]\n",
    "            spread_window = y[t-self.window:t] - hedge_ratio * x[t-self.window:t]\n",
    "            spread_mean = np.mean(spread_window)\n",
    "            spread_std = np.std(spread_window)\n",
    "            upper = spread_mean + spread_std\n",
    "            lower = spread_mean - spread_std\n",
    "\n",
    "            # Check for end-of-day for a potential square-off\n",
    "            is_end_of_day = day_change_indices.iloc[t]\n",
    "\n",
    "            # Check entry\n",
    "            if position == 0:\n",
    "                if spread > upper:\n",
    "                    position = -1  # short spread\n",
    "                    entry_price = spread\n",
    "                    trade_value = abs(y[t] + hedge_ratio * x[t])\n",
    "                    pnl[t] -= trade_value * self.fee_rate\n",
    "                elif spread < lower:\n",
    "                    position = 1  # long spread\n",
    "                    entry_price = spread\n",
    "                    trade_value = abs(y[t] + hedge_ratio * x[t])\n",
    "                    pnl[t] -= trade_value * self.fee_rate\n",
    "            \n",
    "            # Check exit / stop-loss OR End-of-Day Square-off\n",
    "            elif position != 0:\n",
    "                exit_signal = (position == 1 and spread >= spread_mean) or (position == -1 and spread <= spread_mean)\n",
    "                stop_signal = (position == 1 and (spread - entry_price) / entry_price <= -self.stop_loss) or \\\n",
    "                              (position == -1 and (spread - entry_price) / entry_price >= self.stop_loss)\n",
    "                \n",
    "                if exit_signal or stop_signal or is_end_of_day:\n",
    "                    trade_value = abs(y[t] + hedge_ratio * x[t])\n",
    "                    pnl[t] += position * (spread - entry_price)\n",
    "                    pnl[t] -= trade_value * self.fee_rate\n",
    "                    position = 0\n",
    "\n",
    "        return pnl\n",
    "\n",
    "    def run_backtest(self):\n",
    "        daily_equity = [self.initial_capital]\n",
    "        capital = self.initial_capital\n",
    "        all_dates = []\n",
    "        for month, df in self.monthly_dfs.items():\n",
    "            month_dates = df.index\n",
    "            all_dates.extend(month_dates)\n",
    "            month_pnl = np.zeros(len(df))\n",
    "            for s1, s2 in self.pairs:\n",
    "                if s1 in df.columns and s2 in df.columns:\n",
    "                    y = df[s1].values\n",
    "                    x = df[s2].values\n",
    "                    # The fix is here: pass the dates variable\n",
    "                    pair_pnl = self._trade_pair(y, x, month_dates)\n",
    "                    month_pnl += pair_pnl\n",
    "                else:\n",
    "                    print(f\"Warning: Pair ({s1}, {s2}) not found in month {month}. Skipping.\")\n",
    "\n",
    "            for day_pnl in month_pnl:\n",
    "                capital += day_pnl\n",
    "                daily_equity.append(capital)\n",
    "\n",
    "        daily_equity_series = pd.Series(daily_equity[1:], index=all_dates)\n",
    "\n",
    "        returns = daily_equity_series.pct_change().dropna()\n",
    "        sharpe = returns.mean() / returns.std() * np.sqrt(252)\n",
    "        cagr = (daily_equity_series.iloc[-1] / daily_equity_series.iloc[0])**(252 / len(daily_equity_series)) - 1\n",
    "        \n",
    "        return daily_equity_series, sharpe, cagr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4bbb5a76-014f-4c74-95f8-8fb1377d0057",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = [\n",
    "    \"2023-01\",\"2023-02\",\"2023-03\",\"2023-04\",\"2023-05\",\"2023-06\",\n",
    "    \"2023-07\",\"2023-08\",\"2023-09\",\"2023-10\",\"2023-11\",\"2023-12\",\n",
    "    \"2022-01\",\"2022-02\",\"2022-03\",\"2022-04\",\"2022-05\",\"2022-06\",\n",
    "    \"2022-07\",\"2022-08\",\"2022-09\",\"2022-10\",\"2022-11\",\"2022-12\"\n",
    "]\n",
    "\n",
    "monthly_dfs_dict = dict(zip(months, monthly_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1717fc-5e48-49e1-a91d-a19c6b33ed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(\"HDFCBANK\", \"ICICIBANK\")]\n",
    "backtest = PairsTradingBacktest(monthly_dfs_dict, pairs)\n",
    "equity_curve, sharpe, cagr = backtest.run_backtest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a68e89c-b671-4eca-9bbc-fbcada0268df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pairs = coint_pairs\n",
    "backtest = PairsTradingBacktest(monthly_dfs_dict, pairs)\n",
    "equity_curve, sharpe, cagr = backtest.run_backtest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7bf8b5-430c-4c2d-8161-3a5940d170b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sharpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8af130-2694-4b85-90f7-5886437f5376",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cagr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed6a1e3-0726-4ffe-ac94-10f3394301a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot daily equity curve\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(equity_curve.index, equity_curve.values, label=\"Portfolio Equity\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Portfolio Value\")\n",
    "plt.title(f\"Equity Curve\\nSharpe: {sharpe:.2f}, CAGR: {cagr*100:.2f}%\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb865083-f53c-4df7-b7b3-6140a84ee193",
   "metadata": {},
   "source": [
    "# Copula Strategy Code (R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ca247f-a985-49c8-a25c-b4f084866555",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "  library(quantmod)\n",
    "  library(copula)\n",
    "  library(urca)\n",
    "  library(KFAS)\n",
    "  library(pracma)\n",
    "  \n",
    "  # Transaction Fee Parameters\n",
    "  fixed_fee <- 1  # Fixed fee per transaction\n",
    "  percentage_fee <- 0.001  # Percentage of the value of the trade\n",
    "  \n",
    "  # Risk-Free Rate Parameters (e.g., 3% annual rate)\n",
    "  risk_free_annual <- 0.03\n",
    "  \n",
    "  # Function to fetch stock data\n",
    "  extract_df <- function(stocks, start, end, type) {\n",
    "    if (type==\"raw\") {\n",
    "      data1 <- getSymbols(stocks[1], src='yahoo', auto.assign=FALSE, from=start, to=end)\n",
    "      data2 <- getSymbols(stocks[2], src='yahoo', auto.assign=FALSE, from=start, to=end)\n",
    "      \n",
    "      data1 <- Op(data1)\n",
    "      data2 <- Op(data2)\n",
    "      \n",
    "    }\n",
    "    else {\n",
    "      data1 <- getSymbols(stocks[1], src='yahoo', auto.assign=FALSE, from=start, to=end)\n",
    "      data2 <- getSymbols(stocks[2], src='yahoo', auto.assign=FALSE, from=start, to=end)\n",
    "      \n",
    "      data1 <- Ad(data1)\n",
    "      data2 <- Ad(data2)\n",
    "      \n",
    "      data1 <- diff(log(data1))\n",
    "      data2 <- diff(log(data2))\n",
    "      \n",
    "    }\n",
    "    \n",
    "    df <- data.frame(s1=data1, s2=data2)\n",
    "    \n",
    "    return(df)\n",
    "  }\n",
    "  \n",
    "  # Function to compute transaction fee for a trade\n",
    "  calculate_transaction_fee <- function(trade_value) {\n",
    "    fee <- fixed_fee + (trade_value * percentage_fee)\n",
    "    return(fee)\n",
    "  }\n",
    "  \n",
    "  # EM Algorithm\n",
    "  em <- function(data) {\n",
    "    # Transform returns to uniform margins using pobs (pseudo-observations)\n",
    "    u <- pobs(data)\n",
    "    \n",
    "    # Step 2: Initialize the copulas\n",
    "    gaussian_cop <- normalCopula(dim = 2)\n",
    "    gumbel_cop <- gumbelCopula(dim = 2)\n",
    "    clayton_cop <- claytonCopula(dim = 2)\n",
    "    \n",
    "    # Initial weights for each copula\n",
    "    initial_weights <- c(1/3, 1/3, 1/3)\n",
    "    \n",
    "    # Initial parameter estimates for each copula based on the uniform data\n",
    "    initial_params <- list(\n",
    "      gaussian = fitCopula(gaussian_cop, u, method = \"ml\")@estimate,\n",
    "      gumbel = fitCopula(gumbel_cop, u, method = \"ml\")@estimate,\n",
    "      clayton = fitCopula(clayton_cop, u, method = \"ml\")@estimate\n",
    "    )\n",
    "    \n",
    "    # Step 3: Function to calculate the density for each copula\n",
    "    copula_density <- function(u, copula, params) {\n",
    "      copula <- setTheta(copula, params)  # Set the copula parameters\n",
    "      return(dCopula(u, copula))\n",
    "    }\n",
    "    \n",
    "    # Step 4: E-step - Calculate responsibilities (gamma values)\n",
    "    calc_responsibilities <- function(u, weights, params) {\n",
    "      n <- nrow(u)\n",
    "      K <- length(weights)  # Number of copulas\n",
    "      gamma <- matrix(0, n, K)\n",
    "      \n",
    "      # Densities for each copula\n",
    "      densities <- list(\n",
    "        gaussian = copula_density(u, gaussian_cop, params$gaussian),\n",
    "        gumbel = copula_density(u, gumbel_cop, params$gumbel),\n",
    "        clayton = copula_density(u, clayton_cop, params$clayton)\n",
    "      )\n",
    "      \n",
    "      # Calculate responsibilities\n",
    "      for (k in 1:K) {\n",
    "        gamma[, k] <- weights[k] * densities[[k]]\n",
    "      }\n",
    "      \n",
    "      # Normalize to make them probabilities\n",
    "      gamma <- gamma / rowSums(gamma)\n",
    "      return(gamma)\n",
    "    }\n",
    "    \n",
    "    # Step 5: M-step - Update weights and parameters\n",
    "    update_parameters <- function(u, gamma) {\n",
    "      new_weights <- colMeans(gamma)\n",
    "      \n",
    "      new_params <- list()\n",
    "      new_params$gaussian <- fitCopula(gaussian_cop, u, weights = gamma[, 1], method = \"ml\")@estimate\n",
    "      new_params$gumbel <- fitCopula(gumbel_cop, u, weights = gamma[, 2], method = \"ml\")@estimate\n",
    "      new_params$clayton <- fitCopula(clayton_cop, u, weights = gamma[, 3], method = \"ml\")@estimate\n",
    "      \n",
    "      return(list(weights = new_weights, params = new_params))\n",
    "    }\n",
    "    \n",
    "    # Step 6: Function to calculate the log-likelihood of the mixed copula model\n",
    "    calc_log_likelihood <- function(u, weights, params) {\n",
    "      densities <- list(\n",
    "        gaussian = copula_density(u, gaussian_cop, params$gaussian),\n",
    "        gumbel = copula_density(u, gumbel_cop, params$gumbel),\n",
    "        clayton = copula_density(u, clayton_cop, params$clayton)\n",
    "      )\n",
    "      \n",
    "      # Mixed density based on weights\n",
    "      mixed_density <- weights[1] * densities$gaussian + \n",
    "        weights[2] * densities$gumbel + \n",
    "        weights[3] * densities$clayton\n",
    "      \n",
    "      # Log-likelihood\n",
    "      log_likelihood <- sum(log(mixed_density))\n",
    "      return(log_likelihood)\n",
    "    }\n",
    "    \n",
    "    # Step 7: EM algorithm implementation\n",
    "    em_algorithm <- function(u, initial_weights, initial_params, tol = 1e-6, max_iter = 100) {\n",
    "      weights <- initial_weights\n",
    "      params <- initial_params\n",
    "      log_likelihoods <- c()\n",
    "      \n",
    "      for (iter in 1:max_iter) {\n",
    "        # E-step\n",
    "        gamma <- calc_responsibilities(u, weights, params)\n",
    "        \n",
    "        # M-step\n",
    "        updates <- update_parameters(u, gamma)\n",
    "        weights <- updates$weights\n",
    "        params <- updates$params\n",
    "        \n",
    "        # Calculate log-likelihood\n",
    "        log_likelihood <- calc_log_likelihood(u, weights, params)\n",
    "        log_likelihoods <- c(log_likelihoods, log_likelihood)\n",
    "        \n",
    "        # Check for convergence\n",
    "        if (iter > 1 && abs(log_likelihoods[iter] - log_likelihoods[iter - 1]) < tol) {\n",
    "          cat(\"Convergence reached at iteration:\", iter, \"\\n\")\n",
    "          break\n",
    "        }\n",
    "      }\n",
    "      \n",
    "      return(list(weights = weights, params = params, log_likelihood = log_likelihoods))\n",
    "    }\n",
    "    \n",
    "    # Run the EM algorithm\n",
    "    result <- em_algorithm(u, initial_weights, initial_params)\n",
    "    \n",
    "    \n",
    "    # Calculate final AIC using the last log-likelihood and optimized weights\n",
    "    final_log_likelihood <- calc_log_likelihood(u, result$weights, result$params)\n",
    "    num_params <- 3  # Three weights to estimate\n",
    "    final_aic <- -2 * final_log_likelihood + 2 * num_params\n",
    "    \n",
    "    # Return results\n",
    "    return(list(\n",
    "      weights = result$weights,\n",
    "      final_params = result$params,\n",
    "      logLik = final_log_likelihood\n",
    "    ))\n",
    "  }\n",
    "  \n",
    "  # Rolling function with transaction fee, volatility, and Sharpe ratio including risk-free rate\n",
    "  rolling <- function(stocks, total_df, test_df, p1, p2, window, multiple) {\n",
    "    \n",
    "    cointegration_test <- function(df) {\n",
    "      jtest <- ca.jo(df, type = \"trace\", ecdet = \"const\", K = 2)\n",
    "      summary(jtest)\n",
    "      hedge_ratio <- -jtest@V[, 1][2] / jtest@V[, 1][1]\n",
    "      return(hedge_ratio)\n",
    "    }\n",
    "    \n",
    "    conditional_df <- data.frame(c1=numeric(), c2=numeric())\n",
    "    \n",
    "    money <- 1\n",
    "    position <- 0\n",
    "    stocks_of_s1 <- 0\n",
    "    stocks_of_s2 <- 0\n",
    "    equity <- c(money)\n",
    "    transaction_fees <- c()\n",
    "    return_matrix <- matrix(, nrow=nrow(test_df), ncol=5)\n",
    "    \n",
    "    weights <- em(tail(total_df, n=window))$weights\n",
    "    u <- pobs(tail(total_df, n=window))\n",
    "    \n",
    "    cop <- mixCopula(c(gaussian_copula(u)$copula, \n",
    "                       gumbel_copula(u)$copula, \n",
    "                       clayton_copula(u)$copula), \n",
    "                     weights)\n",
    "    \n",
    "    cop1 <- fitCopula(normalCopula(), u, \"ml\")@copula\n",
    "    cop2 <- fitCopula(claytonCopula(), u, \"ml\")@copula\n",
    "    cop3 <- fitCopula(gumbelCopula(), u, \"ml\")@copula\n",
    "    \n",
    "    conditional_copula1 <- cCopula(u, cop1)\n",
    "    conditional_copula2 <- cCopula(u, cop2)\n",
    "    conditional_copula3 <- cCopula(u, cop3)\n",
    "    \n",
    "    new_matrix <- matrix(, nrow=nrow(conditional_copula1), ncol=2)\n",
    "    new_matrix[, 1] <- weights[1]*conditional_copula1[, 1] + weights[2]*conditional_copula2[, 1] + weights[3]*conditional_copula3[, 1]\n",
    "    new_matrix[, 2] <- weights[1]*conditional_copula1[, 2] + weights[2]*conditional_copula2[, 2] + weights[3]*conditional_copula3[, 2]\n",
    "    \n",
    "    df_c <- as.data.frame(new_matrix)\n",
    "    \n",
    "    conditional_df <- rbind(conditional_df, df_c)\n",
    "    \n",
    "    count <- 0\n",
    "    time_fitted <- 1\n",
    "    \n",
    "    for (i in 2:nrow(test_df)) {\n",
    "      if (count >= window) {\n",
    "        weights <- em(tail(total_df, n=window))$weights\n",
    "        u <- pobs(tail(total_df, n=window))\n",
    "        \n",
    "        cop <- mixCopula(c(gaussian_copula(u)$copula, \n",
    "                           gumbel_copula(u)$copula, \n",
    "                           clayton_copula(u)$copula), \n",
    "                         weights)\n",
    "        cop1 <- fitCopula(normalCopula(), u, \"ml\")@copula\n",
    "        cop2 <- fitCopula(claytonCopula(), u, \"ml\")@copula\n",
    "        cop3 <- fitCopula(gumbelCopula(), u, \"ml\")@copula\n",
    "        \n",
    "        conditional_copula1 <- cCopula(u, cop1)\n",
    "        conditional_copula2 <- cCopula(u, cop2)\n",
    "        conditional_copula3 <- cCopula(u, cop3)\n",
    "        \n",
    "        new_matrix <- matrix(, nrow=nrow(conditional_copula1), ncol=2)\n",
    "        new_matrix[, 1] <- weights[1]*conditional_copula1[, 1] + weights[2]*conditional_copula2[, 1] + weights[3]*conditional_copula3[, 1]\n",
    "        new_matrix[, 2] <- weights[1]*conditional_copula1[, 2] + weights[2]*conditional_copula2[, 2] + weights[3]*conditional_copula3[, 2]\n",
    "        \n",
    "        df_c <- as.data.frame(new_matrix)\n",
    "        \n",
    "        conditional_df <- rbind(conditional_df, df_c)\n",
    "        \n",
    "        count <- 0\n",
    "        time_fitted <- time_fitted + 1\n",
    "        \n",
    "      } else {\n",
    "        count <- count + 1\n",
    "      }\n",
    "      \n",
    "      current_return1 <- (test_df[i, 1] / test_df[i-1, 1]) - 1\n",
    "      current_return2 <- (test_df[i, 2] / test_df[i-1, 2]) - 1\n",
    "      new_row <- data.frame(s1=current_return1, s2=current_return2)\n",
    "      colnames(new_row) <- colnames(total_df)\n",
    "      total_df <- rbind(total_df, new_row)\n",
    "      \n",
    "      hedge_ratio <- cointegration_test(tail(total_df, n=window))\n",
    "      mat <- pobs(tail(total_df, n=window))\n",
    "      \n",
    "      condition1 <- conditional_df[i-1, 1]\n",
    "      condition2 <- conditional_df[i-1, 2]\n",
    "      \n",
    "      return_matrix[i, 1] <- condition1\n",
    "      return_matrix[i, 2] <- condition2\n",
    "      return_matrix[i, 3] <- hedge_ratio\n",
    "      #return_matrix[i, 4] <- logLik(compare_copulas(tail(total_df, n=window)))\n",
    "      \n",
    "      # Opening rules\n",
    "      if ((condition1 > 0.5 && condition2 > 0.5) && position != 0) {\n",
    "        trade_value <- as.numeric(abs(stocks_of_s1 * test_df[i, 1]) + abs(stocks_of_s2 * test_df[i, 2]))\n",
    "        fee <- calculate_transaction_fee(trade_value)\n",
    "        money <- as.numeric(money + as.numeric((stocks_of_s1 * test_df[i, 1]) + (stocks_of_s2 * test_df[i, 2])) - fee)\n",
    "        position <- 0\n",
    "        stocks_of_s1 <- 0\n",
    "        stocks_of_s2 <- 0\n",
    "        equity <- c(equity, money)\n",
    "      }\n",
    "      else if ((condition1 <= p1 && condition2 >= p2) && position == 0) {\n",
    "        trade_value <- as.numeric((test_df[i, 1] * multiple) + (hedge_ratio * multiple * test_df[i, 2]))\n",
    "        fee <- calculate_transaction_fee(trade_value)\n",
    "        stocks_of_s1 <- stocks_of_s1 + (1 * multiple)\n",
    "        stocks_of_s2 <- stocks_of_s2 - (hedge_ratio * multiple)\n",
    "        money <- as.numeric(money - (test_df[i, 1] * multiple) + (hedge_ratio * multiple * test_df[i, 2]))\n",
    "        position <- 1\n",
    "        transaction_fees <- c(transaction_fees, fee)\n",
    "        equity <- c(equity, money)\n",
    "      } \n",
    "      else if ((condition1 >= p2 && condition2 <= p1) && position == 0) {\n",
    "        trade_value <- as.numeric((test_df[i, 1] * multiple) + (hedge_ratio * multiple * test_df[i, 2]))\n",
    "        fee <- calculate_transaction_fee(trade_value)\n",
    "        stocks_of_s1 <- stocks_of_s1 - (1 * multiple)\n",
    "        stocks_of_s2 <- stocks_of_s2 + (hedge_ratio * multiple)\n",
    "        money <- as.numeric(money + (test_df[i, 1] * multiple) - (hedge_ratio * multiple * test_df[i, 2]))\n",
    "        position <- -1\n",
    "        transaction_fees <- c(transaction_fees, fee)\n",
    "        equity <- c(equity, money)\n",
    "      }\n",
    "      return_matrix[i, 4] <- stocks_of_s1\n",
    "      return_matrix[i, 5] <- stocks_of_s2\n",
    "    }\n",
    "    \n",
    "    # Calculate volatility (standard deviation of returns)\n",
    "    daily_returns <- diff(equity) / head(equity, -1)\n",
    "    volatility <- sd(daily_returns)\n",
    "    \n",
    "    CAGR <- sqrt(money) - 1 \n",
    "    \n",
    "    # Calculate Sharpe Ratio (with risk-free rate)\n",
    "    sharpe_ratio <- CAGR * sqrt(252) / volatility\n",
    "    \n",
    "    return(list(\n",
    "      returns = money,\n",
    "      transaction_fees = transaction_fees,\n",
    "      volatility = volatility,\n",
    "      cagr = CAGR,\n",
    "      equity = equity,\n",
    "      times = time_fitted,\n",
    "      sharpe = sharpe_ratio,\n",
    "      daily = daily_returns,\n",
    "      mat = return_matrix\n",
    "    ))\n",
    "    \n",
    "  }\n",
    "  \n",
    "  \n",
    "  selected_pairs <- list(\n",
    "    c(\"AMZN\", \"ADBE\"), c(\"V\", \"ACN\"), c(\"V\", \"ADBE\"), c(\"MA\", \"LIN\"),\n",
    "    c(\"V\", \"PYPL\"), c(\"HON\", \"LIN\"), c(\"MA\", \"DHR\"), c(\"NEE\", \"LIN\"),\n",
    "    c(\"AMZN\", \"NFLX\"), c(\"HD\", \"TXN\"), c(\"MA\", \"COST\"), c(\"WMT\", \"LIN\"),\n",
    "    c(\"V\", \"COST\"), c(\"AMZN\", \"PYPL\"), c(\"WMT\", \"ACN\"), c(\"GOOGL\", \"ADBE\"),\n",
    "    c(\"HD\", \"LIN\"), c(\"HD\", \"MA\"), c(\"HD\", \"ADBE\"), c(\"NFLX\", \"ADBE\"),\n",
    "    c(\"HON\", \"TXN\"), c(\"MCD\", \"LIN\"), c(\"UNH\", \"ADBE\"), c(\"MCD\", \"NEE\"),\n",
    "    c(\"AMZN\", \"GOOGL\"), c(\"NKE\", \"COST\"), c(\"ACN\", \"COST\"), c(\"HON\", \"DHR\"),\n",
    "    c(\"AAPL\", \"MSFT\"), c(\"MCD\", \"ACN\"), c(\"UNH\", \"NFLX\"), c(\"WMT\", \"COST\"),\n",
    "    c(\"CSCO\", \"ACN\"), c(\"HD\", \"COST\"), c(\"CSCO\", \"LIN\"), c(\"MCD\", \"DHR\"),\n",
    "    c(\"CSCO\", \"HON\"), c(\"NKE\", \"NEE\"), c(\"MCD\", \"UNP\"), c(\"CSCO\", \"DHR\"),\n",
    "    c(\"WMT\", \"TXN\"), c(\"MRK\", \"LLY\"), c(\"HON\", \"COST\"), c(\"NKE\", \"ACN\"),\n",
    "    c(\"GOOGL\", \"NFLX\"), c(\"NKE\", \"LIN\")\n",
    "  )\n",
    "  \n",
    "  \n",
    "  \n",
    "  test_start <- '2021-01-01'\n",
    "  test_end <- '2023-01-01'\n",
    "  buffer_start <- '2019-01-01'\n",
    "  buffer_end <- '2020-12-31'\n",
    "  p1 <- 0.1\n",
    "  p2 <- 0.9\n",
    "  window <- 250\n",
    "  multiple <- 36.5\n",
    "  \n",
    "  \n",
    "  final_df <- data.frame(stock1=character(), stock2=character(), volatility=numeric(), sharpe_ratio=numeric(), cagr=numeric())\n",
    "  \n",
    "  for(pair in selected_pairs){\n",
    "    total_df <- extract_df(pair, buffer_start, buffer_end, \"returns\")\n",
    "    test_df <- extract_df(pair, test_start, test_end, \"raw\")\n",
    "    \n",
    "    results <- rolling(pair, total_df, test_df, p1, p2, window, multiple)\n",
    "    \n",
    "    temp_df <- data.frame(\n",
    "      stock1=pair[1],\n",
    "      stock2=pair[2],\n",
    "      volatility=results$volatility,\n",
    "      sharpe_ratio=results$sharpe,\n",
    "      cagr=results$cagr\n",
    "    )\n",
    "    \n",
    "    final_df <- rbind(final_df, temp_df)\n",
    "  }\n",
    "  \n",
    "  print(final_df)\n",
    "  \n",
    "  # Export the data frame to a CSV file\n",
    "  #write.csv(final_df, file = \"Strategy Result.csv\", row.names = FALSE)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
